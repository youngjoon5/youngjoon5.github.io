<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Young Joon Oh</title>

    <!-- Bootstrap Core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Theme CSS -->
    <link href="css/clean-blog.min.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

 
    <!-- MathJax for LaTeX -->    
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>

<body>

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    Menu <i class="fa fa-bars"></i>
                </button>
                <a class="navbar-brand" href="index.html">Home</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="about.html">About</a>
                    </li>
                    <li>
                        <a href="research.html">My Research</a>
                    </li>
                    <li>
                        <a href="post.html">Computational Social Science Post</a>
                    </li>
                    <li>
                        <a href="teaching.html">Teaching</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>





    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header class="intro-header" style="background-image: url('img/fall1.jpg')">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <div class="post-heading">
                        <h1>Computational Social Science Post</h1>
                        <h2 class="subheading">Tips on R, Netlogo, and Python</h2>
                       <!--  <span class="meta"><i>&emsp;Posted on Dec.1, 2020</i></span>   -->
                    </div>
                </div>
            </div>
        </div>
    </header>


 

  <!--For making Rmarkdown style <code>  
        <style type="text/css">
            code {
             color: inherit;
             background-color: rgba(0, 0, 0, 0.04);
                  }
        </style>

   --> 



     <!--                      Post Content                   -->
    <!--                      Post Content                   -->
   <!--                      Post Content                   -->







<article>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    

                    <div id="css1"><!-- id for refering this part   -->


                    <h1> The Important thing is whether It’s More Intuitive : Bayesian or Frequentist</h1>
                       <span class="meta"><i>&emsp;Posted on Feb.20, 2025</i></span>
                    
                    
                    <br></br>

                    <div style="text-align: center;">  <!-- place this picture in the center   -->
                          <img src="img/post/bayes.png" style="width:60%">
                    </div>
                    
                    <br></br>

                    
                    <h3>The Concept That Lasts Only a Week in Your Mind: Confidence Intervals</h3>

                    <P> If asked, “What’s the biggest advantage of learning Bayesian statistics?” my quick answer would be: 
                        “You can finally let go of the vague and often perplexing concept of <em>confidence intervals</em>.”
                        <br>
                        One of the earliest challenges for students learning statistical methods is grasping confidence intervals. 
                        Because the concept isn’t intuitive, many forget it soon after their exam(probably within a week). 
                        Typically, the only thing they recall is the number <strong>95%</strong>.<br>
                        
                        Students are taught that <strong>“95% of confidence intervals contain the true mean,”</strong> 
                        yet what they actually remember is <strong>“There’s a 95% chance the true mean lies within the interval.”</strong> 
                        While this latter interpretation is technically wrong in the frequentist framework, it’s far more intuitive and aligns with how we naturally think. 
                        Despite efforts to teach the formal definition, what sticks in students’ minds is the <strong> more intuitive version</strong>.
                    </P>
                    <p>   
                        If this resonates with your own experience, then I say:<br>
                        
                        <strong>“Welcome to Bayesian statistics.”</strong>
                        </P>

                    
                    
                    <h3>Why Bayesian Statistics Feels More Intuitive</h3>

                    <p> When we say, “There’s a 95% probability that a parameter lies in the interval,” 
                        we’re talking about the <strong>credible interval</strong> in Bayesian statistics. 
                        Although it serves a function similar to the confidence interval, its name and interpretation differ.</p>
                    
                    <p>
                        What causes this difference? <br>
                        It largely stems from the frequentist viewpoint. 
                        In frequentist statistics, probabilities arise from the data itself, under the assumption that the dataset collected is one of many potential outcomes. 
                        Analyzing parameters and testing hypotheses with that data is straightforward, 
                        but interpreting those results requires revisiting the probabilistic nature of the dataset. 
                        This back-and-forth hampers intuitive understanding.</p>
                    
                    <p>
                        By contrast, Bayesian statistics directly assigns probabilities to parameters. 
                        The <strong>prior distribution</strong> already contains a probability description of the parameter, 
                        and the <strong>posterior distribution</strong> is the revised version of the prior, updated to incorporate the <strong>observed data</strong>. 
                        Unlike frequentist methods, Bayesian interpretation does <em>not demand toggling</em> between data probabilities and parameter probabilities.
                    
                    </p>
                    
                    
                  

                    <h3>The Big Difference that Intuitiveness Can Make : Decision-Making</h3>

                    <p> Imagine assessing a new treatment (<strong>T</strong>) against a standard treatment (<strong>S</strong>) for a certain disease, 
                        with data from 10 patients in each group: </p>

                    <ul>
                        <li dir="auto"><strong>New Treatment (T) : </strong> 8 out of 10 patients show improvement (Pt = 0.8) </li>
                        <li dir="auto"><strong>Standard Treatment (S) : </strong> 7 out of 10 patients show improvement (Ps = 0.7) </li>

                    </ul>

                    
                    <p><strong>Frequentist Interpretation</strong><br>
                        Using the standard error formula:<br>
                        $$
                        SE = \sqrt{\frac{0.8 \times 0.2}{10} + \frac{0.7 \times 0.3}{10}} = 0.183
                        $$
                        
                        We build a 95% confidence interval (CI) for the difference, Delta = Pt - Ps:
                        $$
                        \Delta \pm 1.96 \times 0.183 \;\longrightarrow\; (-0.26,\, 0.46)
                        $$

                        
                    Because this interval includes zero, the conclusion is that there’s <strong>no statistically significant difference</strong> between the treatments.

                    </p>


                    <p><strong>Bayesian Interpretation</strong><br>
                        Under the Bayesian framework, 
                        we use a Beta distribution as the prior because the treatment proportion(data) has a binary outcome (success/failure), 
                        which follows a binomial distribution. The Beta distribution is the conjugate prior for the binomial distribution. 
                        This <strong>conjugacy</strong> means that when the prior is a Beta distribution and the observed data follows a binomial distribution, 
                        the updated posterior will also have the form of a Beta distribution.  
                        Since we have no existing knowledge about the treatments, we pick an <strong>uninformative prior</strong>, 
                        Beta(1,1) (<em>uniform distribution</em>). When data is applied to the prior :
                        $$
                        p_t​∣D∼Beta(1+8,1+2)=Beta(9,3)
                        $$
                        $$
                        p_s​∣D∼Beta(1+7,1+3)=Beta(8,4)
                        $$


                        Then we calculate the <strong>posterior distribution</strong> of Delta using R:
                        
                        <pre><code> # Monte Carlo Simulation for Posterior
N <- 10000

pt <- rbeta(N, 9, 3)
ps <- rbeta(N, 8, 4) 

delta <- pt - ps

# 2.5th and 97.5th percentiles
credible_interval <- quantile(delta, probs = c(0.025, 0.975))

print(credible_interval)

>>
      2.5%      97.5% 
-0.2733856     0.4263714
</pre></code>



                    Similar to a confidence interval, the <strong>credible interval</strong> includes zero. 
                    Thus, we arrive at the same statistical conclusion: <strong>“With 95% probability, the two treatments do not differ significantly.”</strong>
                </p>


                <p> In Bayesian statistics, since probabilities are directly assigned to parameters, we can ask: 
                    Even if two treatments are statistically indistinguishable, what is the probability that, 
                    in a simulation comparing a group receiving the new treatment to one receiving the standard treatment, the new treatment performs better?<br> 
                    In other words, what is the probability that Δ>0 ?


                    <pre><code>mean(delta > 0)
>>
0.6878 </pre></code>
                    
                    
                    There is roughly a 69% chance that the new treatment outperforms the standard one. 
                    This probability can greatly influence decision-making, particularly when decisions are binary (yes or no). 
                    Although both Frequentist and Bayesian methods may yield similar statistical conclusions, 
                    <strong>the extra insight (or decision-relevant information) from Bayesian analysis</strong> can lead to entirely different decisions.

                    </p>



                <h3>Defending the “Less Intuitive” Prior in Bayesian Thinking</h3>

                <p> In the context of decision-making, it might be more fitting to call the subject <strong>Bayesian reasoning</strong> rather than Bayesian statistics. 
                    Regardless of the label, the core process remains unchanged: the prior distribution is updated with observed data to create the posterior distribution.<br>

                    A recurring debate in Bayesian methods centers on selecting the appropriate prior. Which one should we choose? 
                    While this decision is crucial, it also raises the question of why priors often seem so challenging. 
                    Like confidence intervals, <strong>priors can feel less intuitive and difficult to grasp</strong>.
                </p>
                
                <p>   
                    Why is this so?<br>
                    At its essence, a prior represents our existing knowledge or belief. 
                    For instance, when asked, “Do ghosts exist?” the answer is typically a simple yes or no—a binomial outcome. 
                    A question like “Does the advancement of AI contribute to democracy?” can elicit a range of responses—from “yes” to “no,” 
                    with varying degrees of certainty—forming a multinomial framework. 
                    Without imposing a binomial or multinomial structure, converting people’s beliefs into a statistical distribution becomes 
                    a daunting task that only grows more complex with more intricate questions.
                </p>
                
                <p>
                    Moreover, can we fully trust our knowledge and beliefs? 
                    Humans tend to overestimate dramatic risks while underestimating common ones, and the way information is presented—known as framing effects—can heavily influence our judgments.<br>
                    
                    Another concern is the reliability of updating priors with new information. 
                    According to Madsen (2016), American voters are more likely to support a policy based on whether particular political figures endorse or criticize it. 
                    In other words, voters’ priors can affect how they interpret new data.<br>
                    
                    These challenges arise from the inherently less intuitive nature of priors. 
                    Ultimately, priors are subjective probability assessments that naturally vary among individuals, groups, and researchers—and that’s exactly how it should be. 
                    The idea that everyone shares the same knowledge and beliefs is not only unrealistic, it would be quite unsettling.
                </p>

                <p>
                    To illustrate priors further, consider the <strong>paradox of the heap</strong>. <br>
                    Imagine a large heap of sand. Removing one grain still leaves you with a heap, but if you continue removing grains until only one remains, is it still a heap? 
                    There is no clear boundary, and different people might answer this question differently.<br>
                    
                    If we abandon the search for clear boundaries and change our perspective, the paradox of the heap ceases to be a paradox. 
                    Picture a million grains of sand—your confidence that this forms a heap is high. 
                    As you remove one grain at a time, your confidence diminishes gradually. 
                    In this way, belief is not binary but a matter of <strong>graded confidence</strong> or <strong>degrees of belief</strong>. 
                    When we think of belief as akin to the confidence that a pile of sand is a heap—decreasing gradually 
                    with each removed grain—priors can indeed become <strong>more intuitive</strong>. 
                    A belief about any issue can fluctuate like temperature: new data may raise or lower our confidence incrementally. <br>

                    Different priors(as graded beliefs) can lead to similar or divergent posteriors based on the observed data. 
                    When the evidence is strong, the posterior tends to closely follow the data. 
                    However, if the balance between the prior and the data is maintained, the posterior remains influenced by the priors. 
                    Ultimately, the most appropriate prior is the one that enhances future predictions. 
                    The strength of our subjective priors depends on their ability to predict future outcomes — a process at the very heart of Bayesian reasoning.
                </p>
                
                <p>
                    Bayesian reasoning helps us find <strong>better priors</strong>—echoing George Box’s timeless remark:<br>
                    <strong>"All models are wrong, but some are useful."</strong>
                </p>



            <br>
            <h3 data-heading="Reference" dir="auto">Reference</h3>
                <p dir="auto">Chivers, T. (2024). <em>Everything is Predictable: How Bayesian Statistics Explain Our World</em>.  Simon and Schuster.</p>
                <p dir="auto">Fornacon-Wood, I., Mistry, H., Johnson-Hart, C., Faivre-Finn, C., O’Connor, J. P. B., & Price, G. J. (2022). 
                     <em>Understanding the Differences Between Bayesian and Frequentist Statistics</em>. International Journal of Radiation Oncology, Biology, Physics, 112(5), 1076–1082.</p>
                <p dir="auto">Madsen, J. K. (2016). <em>Trump supported it?! A Bayesian source credibility model applied to appeals to specific American presidential candidates’ opinions</em>. 
                    In Proceedings of the Annual Meeting of the Cognitive Science Society (Vol. 38).</p>



                     




                    




 








<!-- blockquote 
<blockquote>The dreams of yesterday are the hopes of today and the reality of tomorrow. Science has not yet mastered prophecy. We predict too much for the next year and yet far too little for the next ten.</blockquote>

<mark> test</mark>
-->




<!-- Begin : pager back to Computational social science post -->

<ul class="pager">
                        <li class="next">
                            <a href="post.html">Return to Post List &rarr;</a>
                        </li>
                    </ul>
                
<!-- End : pager back to Computational social science post -->
                  


                    <br></br>

                  </div>
                </div>
            </div>
        </div>


    </article>

    <hr>




   






    <!--            Footer            -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <ul class="list-inline text-center">
                                                <li>
                            <a href="https://www.youtube.com/user/yjoon5">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-youtube fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/youngjoon5">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    </ul>
                    <p class="copyright text-muted">Copyright &copy; Start Bootstrap 2016</p>
                </div>
            </div>
        </div>
    </footer>







    <!-- jQuery -->
    <script src="vendor/jquery/jquery.min.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="vendor/bootstrap/js/bootstrap.min.js"></script>

    <!-- Contact Form JavaScript -->
    <script src="js/jqBootstrapValidation.js"></script>
    <script src="js/contact_me.js"></script>

    <!-- Theme JavaScript -->
    <script src="js/clean-blog.min.js"></script>

</body>

</html>
